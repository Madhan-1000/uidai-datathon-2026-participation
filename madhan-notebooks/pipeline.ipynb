{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c63a314",
   "metadata": {},
   "source": [
    "# End-to-End Data Cleaning and Normalization Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e544721",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "This first section imports all necessary libraries and defines the paths for the entire pipeline. It sets up the root directory and specifies input, intermediate, and final output locations. This ensures the notebook is self-contained and can be run from top to bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82bfb70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\INDIAN\\OneDrive\\Documents\\PRO__\\uidai-datathon-2026-participation\n",
      "\\n--- Directories Initialized ---\n",
      "Intermediate (100k cleaned): C:\\Users\\INDIAN\\OneDrive\\Documents\\PRO__\\uidai-datathon-2026-participation\\cleaned-100k\n",
      "Intermediate (major rules): C:\\Users\\INDIAN\\OneDrive\\Documents\\PRO__\\uidai-datathon-2026-participation\\cleaned_major_rules\n",
      "Final Output: C:\\Users\\INDIAN\\OneDrive\\Documents\\PRO__\\uidai-datathon-2026-participation\\final_pipeline_output\n",
      "Logs: C:\\Users\\INDIAN\\OneDrive\\Documents\\PRO__\\uidai-datathon-2026-participation\\final_pipeline_output\\logs\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "# --- PATH DEFINITIONS ---\n",
    "\n",
    "# Get the absolute path of the project root by going up from the notebook's location\n",
    "ROOT = Path(os.getcwd()).resolve().parent\n",
    "print(f\"Project Root: {ROOT}\")\n",
    "\n",
    "# Source data\n",
    "DATA_ROOT = ROOT / \"data\"\n",
    "\n",
    "# Intermediate directories for each cleaning step\n",
    "CLEANED_100K_DIR = ROOT / \"cleaned-100k\"\n",
    "CLEANED_MAJOR_RULES_DIR = ROOT / \"cleaned_major_rules\"\n",
    "\n",
    "# Final output directory\n",
    "FINAL_OUTPUT_DIR = ROOT / \"final_pipeline_output\"\n",
    "LOGS_DIR = FINAL_OUTPUT_DIR / \"logs\"\n",
    "\n",
    "# External data files\n",
    "LGD_JSON_PATH = ROOT / \"India-State-District.json\"\n",
    "\n",
    "# --- CLEANUP AND SETUP ---\n",
    "\n",
    "# For a clean run, remove intermediate and final directories if they exist\n",
    "if CLEANED_100K_DIR.exists():\n",
    "    shutil.rmtree(CLEANED_100K_DIR)\n",
    "    print(f\"Removed existing directory: {CLEANED_100K_DIR}\")\n",
    "\n",
    "if CLEANED_MAJOR_RULES_DIR.exists():\n",
    "    shutil.rmtree(CLEANED_MAJOR_RULES_DIR)\n",
    "    print(f\"Removed existing directory: {CLEANED_MAJOR_RULES_DIR}\")\n",
    "\n",
    "if FINAL_OUTPUT_DIR.exists():\n",
    "    shutil.rmtree(FINAL_OUTPUT_DIR)\n",
    "    print(f\"Removed existing directory: {FINAL_OUTPUT_DIR}\")\n",
    "\n",
    "# Create fresh directories\n",
    "CLEANED_100K_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CLEANED_MAJOR_RULES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FINAL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\\\n--- Directories Initialized ---\")\n",
    "print(f\"Intermediate (100k cleaned): {CLEANED_100K_DIR}\")\n",
    "print(f\"Intermediate (major rules): {CLEANED_MAJOR_RULES_DIR}\")\n",
    "print(f\"Final Output: {FINAL_OUTPUT_DIR}\")\n",
    "print(f\"Logs: {LOGS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a1fb3",
   "metadata": {},
   "source": [
    "## 2. Step 1: Remove '100000' Pincode Entries\n",
    "\n",
    "This step takes the raw data from the `data/` directory, iterates through each CSV file, and removes any rows where the `pincode` column has the value '100000'. The resulting cleaned files are saved to the first intermediate directory, `cleaned-100k/`, preserving the original sub-folder structure (`biometric`, `demographic`, `enrolment`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8fd68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Removing '100000' Pincode Values ---\n",
      "Processing: biometric_0_500000.csv\n",
      "  - Rows removed: 0\n",
      "Processing: biometric_1000000_1500000.csv\n",
      "  - Rows removed: 0\n",
      "Processing: biometric_1500000_1861108.csv\n",
      "  - Rows removed: 0\n",
      "Processing: biometric_500000_1000000.csv\n",
      "  - Rows removed: 0\n",
      "Processing: demographic_0_500000.csv\n",
      "  - Rows removed: 0\n",
      "Processing: demographic_1000000_1500000.csv\n",
      "  - Rows removed: 0\n",
      "Processing: demographic_1500000_2000000.csv\n",
      "  - Rows removed: 0\n",
      "Processing: demographic_2000000_2071700.csv\n",
      "  - Rows removed: 1\n",
      "Processing: demographic_500000_1000000.csv\n",
      "  - Rows removed: 1\n",
      "Processing: enrolment_0_500000.csv\n",
      "  - Rows removed: 9\n",
      "Processing: enrolment_1000000_1006029.csv\n",
      "  - Rows removed: 0\n",
      "Processing: enrolment_500000_1000000.csv\n",
      "  - Rows removed: 13\n",
      "\\n--- Step 1 Complete ---\n"
     ]
    }
   ],
   "source": [
    "def remove_100k_values(input_root: Path, output_root: Path):\n",
    "    \"\"\"\n",
    "    Reads all CSVs from input_root, removes rows where 'pincode' is '100000',\n",
    "    and saves them to output_root, mirroring the directory structure.\n",
    "    \"\"\"\n",
    "    print(\"--- Step 1: Removing '100000' Pincode Values ---\")\n",
    "    \n",
    "    csv_files = sorted(list(input_root.rglob(\"*.csv\")))\n",
    "    if not csv_files:\n",
    "        print(f\"[WARN] No CSV files found in {input_root}\")\n",
    "        return\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Determine output path\n",
    "            relative_path = csv_file.relative_to(input_root)\n",
    "            output_path = output_root / relative_path\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            print(f\"Processing: {csv_file.name}\")\n",
    "\n",
    "            # Read, filter, and save\n",
    "            df = pd.read_csv(csv_file, dtype=str)\n",
    "            \n",
    "            # Find pincode column case-insensitively\n",
    "            pincode_col = next((col for col in df.columns if 'pincode' in col.lower()), None)\n",
    "            \n",
    "            if pincode_col:\n",
    "                initial_rows = len(df)\n",
    "                df = df[df[pincode_col] != '100000']\n",
    "                final_rows = len(df)\n",
    "                print(f\"  - Rows removed: {initial_rows - final_rows}\")\n",
    "            else:\n",
    "                print(f\"  - Pincode column not found in {csv_file.name}. Copying as is.\")\n",
    "\n",
    "            df.to_csv(output_path, index=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process {csv_file.name}: {e}\")\n",
    "\n",
    "    print(\"\\\\n--- Step 1 Complete ---\")\n",
    "\n",
    "# Execute the function\n",
    "remove_100k_values(DATA_ROOT, CLEANED_100K_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d909a5",
   "metadata": {},
   "source": [
    "## 3. Step 2: Apply Major State/District Canonicalization\n",
    "\n",
    "This step incorporates the powerful, deterministic rule set from the `Final_changer.py` script. It processes the data from the `cleaned-100k/` directory and applies a vast mapping of state aliases and district-to-state overrides. This is the most significant cleaning step for correcting common, known anomalies (e.g., `Orissa` -> `Odisha`, `Rangareddy` -> `Telangana`). The results are written to the `cleaned_major_rules/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b5bc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Step 2: Applying Major Canonicalization Rules ---\n",
      "Processing: biometric_0_500000.csv\n",
      "Processing: biometric_1000000_1500000.csv\n",
      "Processing: biometric_1500000_1861108.csv\n",
      "Processing: biometric_500000_1000000.csv\n",
      "Processing: demographic_0_500000.csv\n",
      "Processing: demographic_1000000_1500000.csv\n",
      "Processing: demographic_1500000_2000000.csv\n",
      "Processing: demographic_2000000_2071700.csv\n",
      "Processing: demographic_500000_1000000.csv\n",
      "Processing: enrolment_0_500000.csv\n",
      "Processing: enrolment_1000000_1006029.csv\n",
      "Processing: enrolment_500000_1000000.csv\n",
      "\\n--- Step 2 Complete ---\n"
     ]
    }
   ],
   "source": [
    "# This cell contains the core logic adapted from Final_changer.py\n",
    "\n",
    "# --- NORMALIZATION AND RULE DEFINITIONS ---\n",
    "\n",
    "PUNCT_PATTERN = re.compile(r\"[\\\\.,;:/\\\\\\\\\\\"'`\\\\-]+\")\n",
    "SPACE_COLLAPSE = re.compile(r\"\\\\s+\")\n",
    "\n",
    "CANONICAL_STATES = {\n",
    "\t\"andhra pradesh\", \"arunachal pradesh\", \"assam\", \"bihar\", \"chhattisgarh\", \"goa\", \"gujarat\", \"haryana\", \n",
    "\t\"himachal pradesh\", \"jharkhand\", \"karnataka\", \"kerala\", \"madhya pradesh\", \"maharashtra\", \"manipur\", \n",
    "\t\"meghalaya\", \"mizoram\", \"nagaland\", \"odisha\", \"punjab\", \"rajasthan\", \"sikkim\", \"tamil nadu\", \"telangana\", \n",
    "\t\"tripura\", \"uttar pradesh\", \"uttarakhand\", \"west bengal\", \"andaman and nicobar islands\", \"chandigarh\", \n",
    "\t\"dadra and nagar haveli and daman and diu\", \"delhi\", \"jammu and kashmir\", \"ladakh\", \"lakshadweep\", \"puducherry\",\n",
    "}\n",
    "\n",
    "STATE_ALIAS_MAP = {\n",
    "\t\"orissa\": \"odisha\", \"uttaranchal\": \"uttarakhand\", \"tamilnadu\": \"tamil nadu\", \"chhatisgarh\": \"chhattisgarh\",\n",
    "\t\"chattisgarh\": \"chhattisgarh\", \"west bangal\": \"west bengal\", \"west bengli\": \"west bengal\", \n",
    "\t\"westbengal\": \"west bengal\", \"pondicherry\": \"puducherry\", \"daman & diu\": \"dadra and nagar haveli and daman and diu\",\n",
    "\t\"daman and diu\": \"dadra and nagar haveli and daman and diu\", \"dadra & nagar haveli\": \"dadra and nagar haveli and daman and diu\",\n",
    "\t\"jammu & kashmir\": \"jammu and kashmir\", \"jammu and kashmir\": \"jammu and kashmir\",\n",
    "\t\"the dadra and nagar haveli and daman and diu\": \"dadra and nagar haveli and daman and diu\",\n",
    "}\n",
    "\n",
    "LADAKH_DISTRICTS = {\"leh\", \"kargil\"}\n",
    "TELANGANA_DISTRICTS = {\n",
    "\t\"hyderabad\", \"rangareddy\", \"k v rangareddy\", \"k.v. rangareddy\", \"warangal\", \"nalgonda\", \"medak\", \n",
    "\t\"khammam\", \"karimnagar\", \"adilabad\", \"mahabubnagar\", \"nizamabad\",\n",
    "}\n",
    "\n",
    "DISTRICT_FORCE_MAP = {\n",
    "    \"rupnagar\": \"punjab\", \"k.v. rangareddy\": \"telangana\", \"k v rangareddy\": \"telangana\", \"rangareddy\": \"telangana\",\n",
    "    \"dadra and nagar haveli\": \"dadra and nagar haveli and daman and diu\", \"bandipore\": \"jammu and kashmir\",\n",
    "    \"bandipora\": \"jammu and kashmir\", \"punch\": \"jammu and kashmir\", \"poonch\": \"jammu and kashmir\",\n",
    "    \"rajauri\": \"jammu and kashmir\", \"rajouri\": \"jammu and kashmir\", \"bandipur\": \"jammu and kashmir\",\n",
    "    \"kupwara\": \"jammu and kashmir\", \"jammu\": \"jammu and kashmir\", \"kathua\": \"jammu and kashmir\",\n",
    "    \"srinagar\": \"jammu and kashmir\", \"badgam\": \"jammu and kashmir\", \"baramula\": \"jammu and kashmir\",\n",
    "    \"anantnag\": \"jammu and kashmir\", \"udhampur\": \"jammu and kashmir\", \"doda\": \"jammu and kashmir\",\n",
    "    \"pulwama\": \"jammu and kashmir\", \"ganderbal\": \"jammu and kashmir\", \"kulgam\": \"jammu and kashmir\",\n",
    "    \"kishtwar\": \"jammu and kashmir\", \"kargil\": \"ladakh\", \"leh\": \"ladakh\", \"leh (ladakh)\": \"ladakh\",\n",
    "    \"kamrup\": \"assam\", \"hyderabad\": \"telangana\", \"warangal\": \"telangana\", \"nizamabad\": \"telangana\",\n",
    "    \"karimnagar\": \"telangana\", \"nalgonda\": \"telangana\", \"medak\": \"telangana\", \"adilabad\": \"telangana\",\n",
    "    \"mahabubnagar\": \"telangana\", \"khammam\": \"telangana\", \"south andaman\": \"andaman and nicobar islands\",\n",
    "    \"cuddalore\": \"tamil nadu\", \"viluppuram\": \"tamil nadu\", \"pondicherry\": \"puducherry\", \"karikal\": \"puducherry\",\n",
    "    \"karaikal\": \"puducherry\", \"yanam\": \"puducherry\", \"daman\": \"dadra and nagar haveli and daman and diu\",\n",
    "    \"diu\": \"dadra and nagar haveli and daman and diu\", \"nicobar\": \"andaman and nicobar islands\",\n",
    "    \"nicobars\": \"andaman and nicobar islands\", \"north and middle andaman\": \"andaman and nicobar islands\",\n",
    "    \"andamans\": \"andaman and nicobar islands\", \"ahilyanagar\": \"maharashtra\", \"ahmadnagar\": \"maharashtra\",\n",
    "    \"ahmed nagar\": \"maharashtra\", \"ahmednagar\": \"maharashtra\", \"alipurduar\": \"west bengal\",\n",
    "    \"ambedkar nagar\": \"uttar pradesh\", \"ashok nagar\": \"madhya pradesh\", \"ashoknagar\": \"madhya pradesh\",\n",
    "    \"barddhaman\": \"west bengal\", \"bardez\": \"goa\", \"bardhaman\": \"west bengal\", \"bhavnagar\": \"gujarat\",\n",
    "    \"burdwan\": \"west bengal\", \"chamarajanagar\": \"karnataka\", \"chamarajanagar *\": \"karnataka\",\n",
    "    \"chamrajanagar\": \"karnataka\", \"chamrajnagar\": \"karnataka\", \"chatrapati sambhaji nagar\": \"maharashtra\",\n",
    "    \"chhatrapati sambhajinagar\": \"maharashtra\", \"gandhinagar\": \"gujarat\", \"ganganagar\": \"rajasthan\",\n",
    "    \"gautam buddha nagar\": \"uttar pradesh\", \"gautam buddha nagar *\": \"uttar pradesh\", \"gurdaspur\": \"punjab\",\n",
    "    \"harda\": \"madhya pradesh\", \"harda *\": \"madhya pradesh\", \"hardoi\": \"uttar pradesh\", \"hardwar\": \"uttarakhand\",\n",
    "    \"jamnagar\": \"gujarat\", \"jyotiba phule nagar\": \"uttar pradesh\", \"jyotiba phule nagar *\": \"uttar pradesh\",\n",
    "    \"kabeerdham\": \"chhattisgarh\", \"kawardha\": \"chhattisgarh\", \"kanpur nagar\": \"uttar pradesh\",\n",
    "    \"khorda\": \"odisha\", \"khordha\": \"odisha\", \"khordha  *\": \"odisha\", \"khorda  *\": \"odisha\",\n",
    "    \"kushi nagar\": \"uttar pradesh\", \"kushinagar\": \"uttar pradesh\", \"kushinagar *\": \"uttar pradesh\",\n",
    "    \"lohardaga\": \"jharkhand\", \"mahabub nagar\": \"telangana\", \"mahabubnagar\": \"telangana\",\n",
    "    \"aurangabad\": \"maharashtra\", \"bijapur\": \"karnataka\", \"raigarh\": \"chhattisgarh\", \"balrampur\": \"uttar pradesh\",\n",
    "    \"pratapgarh\": \"uttar pradesh\", \"bilaspur\": \"chhattisgarh\"\n",
    "}\n",
    "\n",
    "HAMIRPUR_VALID = {\"himachal pradesh\", \"uttar pradesh\"}\n",
    "\n",
    "def normalize_for_match(value: str) -> str:\n",
    "    if not isinstance(value, str):\n",
    "        return \"\"\n",
    "    s = value.strip().lower()\n",
    "    s = PUNCT_PATTERN.sub(\" \", s)\n",
    "    s = SPACE_COLLAPSE.sub(\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def title_case_clean(value: str) -> str:\n",
    "    return \" \".join(part.capitalize() for part in value.split())\n",
    "\n",
    "def _state_district_columns(columns: list) -> tuple:\n",
    "    state_col = next((c for c in columns if \"state\" in c.lower()), None)\n",
    "    dist_col = next((c for c in columns if \"district\" in c.lower()), None)\n",
    "    return state_col, dist_col\n",
    "\n",
    "def resolve_state_district(state_val: str, dist_val: str, logs: dict) -> tuple:\n",
    "    orig_state, orig_dist = state_val, dist_val\n",
    "    n_state = normalize_for_match(state_val)\n",
    "    n_dist = normalize_for_match(dist_val)\n",
    "    \n",
    "    is_corrected = False\n",
    "\n",
    "    # Apply rules\n",
    "    if n_dist in DISTRICT_FORCE_MAP:\n",
    "        n_state = DISTRICT_FORCE_MAP[n_dist]\n",
    "        is_corrected = True\n",
    "    elif n_dist in LADAKH_DISTRICTS:\n",
    "        n_state = \"ladakh\"\n",
    "        is_corrected = True\n",
    "    elif n_dist in TELANGANA_DISTRICTS:\n",
    "        n_state = \"telangana\"\n",
    "        is_corrected = True\n",
    "    \n",
    "    if n_state in STATE_ALIAS_MAP:\n",
    "        n_state = STATE_ALIAS_MAP[n_state]\n",
    "        is_corrected = True\n",
    "\n",
    "    # Hamirpur collision handling\n",
    "    if n_dist == \"hamirpur\" and n_state not in HAMIRPUR_VALID:\n",
    "        logs['collisions'].append((orig_state, orig_dist))\n",
    "        return orig_state, orig_dist, False # Return original, not corrected\n",
    "\n",
    "    if n_state not in CANONICAL_STATES:\n",
    "        logs['unresolved'].append((orig_state, orig_dist))\n",
    "        return orig_state, orig_dist, False\n",
    "\n",
    "    final_state = title_case_clean(n_state)\n",
    "    final_dist = title_case_clean(n_dist)\n",
    "    \n",
    "    if is_corrected:\n",
    "        logs['corrections'].append((orig_state, orig_dist, final_state, final_dist))\n",
    "\n",
    "    return final_state, final_dist, is_corrected\n",
    "\n",
    "# --- FILE PROCESSING FUNCTION ---\n",
    "\n",
    "def apply_major_rules(input_root: Path, output_root: Path, chunksize: int = 100_000):\n",
    "    print(\"\\\\n--- Step 2: Applying Major Canonicalization Rules ---\")\n",
    "    \n",
    "    csv_files = sorted(list(input_root.rglob(\"*.csv\")))\n",
    "    if not csv_files:\n",
    "        print(f\"[WARN] No CSV files found in {input_root}\")\n",
    "        return\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        rel_path = csv_file.relative_to(input_root)\n",
    "        output_path = output_root / rel_path\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"Processing: {csv_file.name}\")\n",
    "        \n",
    "        # Setup logs for this file\n",
    "        logs = defaultdict(list)\n",
    "        \n",
    "        # Process in chunks\n",
    "        reader = pd.read_csv(csv_file, dtype=str, chunksize=chunksize, on_bad_lines='skip', engine='python')\n",
    "        \n",
    "        is_first_chunk = True\n",
    "        for chunk in reader:\n",
    "            state_col, dist_col = _state_district_columns(chunk.columns)\n",
    "            if not state_col or not dist_col:\n",
    "                chunk.to_csv(output_path, mode='a', index=False, header=is_first_chunk)\n",
    "                is_first_chunk = False\n",
    "                continue\n",
    "\n",
    "            for idx, row in chunk.iterrows():\n",
    "                state, dist, corrected = resolve_state_district(row[state_col], row[dist_col], logs)\n",
    "                chunk.at[idx, state_col] = state\n",
    "                chunk.at[idx, dist_col] = dist\n",
    "            \n",
    "            chunk.to_csv(output_path, mode='a', index=False, header=is_first_chunk)\n",
    "            is_first_chunk = False\n",
    "            \n",
    "    print(\"\\\\n--- Step 2 Complete ---\")\n",
    "\n",
    "# Execute the function\n",
    "apply_major_rules(CLEANED_100K_DIR, CLEANED_MAJOR_RULES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d04106d",
   "metadata": {},
   "source": [
    "## 4. Step 3: LGD-Based Normalization and Validation\n",
    "\n",
    "This is the final and strictest cleaning step. It uses the official Local Government Directory (LGD) as the single source of truth.\n",
    "\n",
    "The logic, adapted from `Normalizing_districts.py`, will:\n",
    "1.  Normalize text fields.\n",
    "2.  Apply known district aliases (e.g., `Bardhaman` -> `Purba Bardhaman`).\n",
    "3.  Force state assignments for post-bifurcation districts (e.g., `Hyderabad` -> `Telangana`).\n",
    "4.  **Crucially, it validates every single row against the LGD master data.**\n",
    "    *   If a district doesn't exist in the LGD, the row is **dropped**.\n",
    "    *   If a district exists but is mapped to the wrong state, the row is **dropped**.\n",
    "    *   If a district is ambiguous (belongs to multiple states) and cannot be resolved, the row is **dropped**.\n",
    "5.  The clean data is written to the `final_pipeline_output/` directory.\n",
    "6.  All dropped rows are logged into **separate files** based on their source category (`biometric`, `demographic`, `enrolment`) in the `final_pipeline_output/logs/` directory, as requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc9db3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Step 3: Applying LGD-Based Normalization and Validation ---\n",
      "LGD Master Loaded: 773 unique districts.\n",
      "Processing: biometric_0_500000.csv (logging to biometric_unresolved.csv)\n",
      "Processing: biometric_1000000_1500000.csv (logging to biometric_unresolved.csv)\n",
      "Processing: biometric_1500000_1861108.csv (logging to biometric_unresolved.csv)\n",
      "Processing: biometric_500000_1000000.csv (logging to biometric_unresolved.csv)\n",
      "Processing: demographic_0_500000.csv (logging to demographic_unresolved.csv)\n",
      "Processing: demographic_1000000_1500000.csv (logging to demographic_unresolved.csv)\n",
      "Processing: demographic_1500000_2000000.csv (logging to demographic_unresolved.csv)\n",
      "Processing: demographic_2000000_2071700.csv (logging to demographic_unresolved.csv)\n",
      "Processing: demographic_500000_1000000.csv (logging to demographic_unresolved.csv)\n",
      "Processing: enrolment_0_500000.csv (logging to enrolment_unresolved.csv)\n",
      "Processing: enrolment_1000000_1006029.csv (logging to enrolment_unresolved.csv)\n",
      "Processing: enrolment_500000_1000000.csv (logging to enrolment_unresolved.csv)\n",
      "\\n--- Step 3 Complete ---\n"
     ]
    }
   ],
   "source": [
    "# This cell contains the core logic adapted from Normalizing_districts.py\n",
    "\n",
    "# --- LGD NORMALIZATION AND RULE DEFINITIONS ---\n",
    "\n",
    "LGD_CANONICAL_STATES = {\n",
    "\t\"Andaman And Nicobar Islands\", \"Andhra Pradesh\", \"Arunachal Pradesh\", \"Assam\", \"Bihar\", \"Chandigarh\", \n",
    "    \"Chhattisgarh\", \"Dadra And Nagar Haveli And Daman And Diu\", \"Delhi\", \"Goa\", \"Gujarat\", \"Haryana\", \n",
    "    \"Himachal Pradesh\", \"Jammu And Kashmir\", \"Jharkhand\", \"Karnataka\", \"Kerala\", \"Ladakh\", \"Lakshadweep\", \n",
    "    \"Madhya Pradesh\", \"Maharashtra\", \"Manipur\", \"Meghalaya\", \"Mizoram\", \"Nagaland\", \"Odisha\", \"Puducherry\", \n",
    "    \"Punjab\", \"Rajasthan\", \"Sikkim\", \"Tamil Nadu\", \"Telangana\", \"Tripura\", \"Uttar Pradesh\", \"Uttarakhand\", \"West Bengal\",\n",
    "}\n",
    "\n",
    "LGD_STATE_ALIAS = {\n",
    "\t\"Orissa\": \"Odisha\", \"Uttaranchal\": \"Uttarakhand\", \"Chattisgarh\": \"Chhattisgarh\", \"Pondicherry\": \"Puducherry\",\n",
    "\t\"Andaman And Nicobar\": \"Andaman And Nicobar Islands\", \"Daman And Diu\": \"Dadra And Nagar Haveli And Daman And Diu\",\n",
    "\t\"Dadra And Nagar Haveli\": \"Dadra And Nagar Haveli And Daman And Diu\",\n",
    "}\n",
    "\n",
    "LGD_DISTRICT_ALIAS = {\n",
    "\t\"Punch\": \"Poonch\", \"Davangere\": \"Davanagere\", \"Tumkur\": \"Tumakuru\", \"Bellary\": \"Ballari\", \"Sundergarh\": \"Sundargarh\",\n",
    "\t\"Baleswar\": \"Balasore\", \"Anugul\": \"Angul\", \"Sabarkantha\": \"Sabar Kantha\", \"Banaskantha\": \"Banas Kantha\",\n",
    "\t\"Panchmahals\": \"Panch Mahals\", \"Maldah\": \"Malda\", \"Bardhaman\": \"Purba Bardhaman\", \"Ahmednagar\": \"Ahilyanagar\",\n",
    "\t\"Aurangabad\": \"Chhatrapati Sambhajinagar\", \"Allahabad\": \"Prayagraj\", \"Faizabad\": \"Ayodhya\",\n",
    "\t\"Villupuram\": \"Viluppuram\", \"Tirupattur\": \"Tirupathur\", \"Tuticorin\": \"Thoothukkudi\", \"West Nimar\": \"Khargone\",\n",
    "\t\"East Nimar\": \"Khandwa\", \"Hoshangabad\": \"Narmadapuram\", \"Y S R\": \"Y.S.R. Kadapa\", \"K V Rangareddy\": \"Ranga Reddy\",\n",
    "\t\"Karim Nagar\": \"Karimnagar\",\n",
    "}\n",
    "\n",
    "LGD_FORCE_STATE_BY_DISTRICT = {\n",
    "\t\"Hyderabad\": \"Telangana\", \"Nizamabad\": \"Telangana\", \"Warangal\": \"Telangana\", \"Adilabad\": \"Telangana\",\n",
    "\t\"Nalgonda\": \"Telangana\", \"Bhadradri Kothagudem\": \"Telangana\", \"Karimnagar\": \"Telangana\", \"Mahabubnagar\": \"Telangana\",\n",
    "\t\"Ranchi\": \"Jharkhand\", \"Hazaribagh\": \"Jharkhand\", \"Dibrugarh\": \"Assam\", \"Leh\": \"Ladakh\", \"Kargil\": \"Ladakh\",\n",
    "}\n",
    "\n",
    "DROP_KEYWORDS= {\n",
    "\t\"Near\", \"Colony\", \"Sector\", \"Phase\", \"Road\", \"Cross\", \"Thana\", \"Hospital\", \"University\", \"Dist \", \"Sub Urban\", \"Suburban\",\n",
    "}\n",
    "\n",
    "def lgd_normalize_text(value) -> str:\n",
    "    if value is None or pd.isna(value): return \"\"\n",
    "    if not isinstance(value, str): value = str(value)\n",
    "    s = value.strip()\n",
    "    if not s: return \"\"\n",
    "    s = s.replace(\".\", \"\")\n",
    "    s = s.replace(\"&\", \"And\")\n",
    "    s = \" \".join(s.split())\n",
    "    return s.title()\n",
    "\n",
    "def load_lgd_master(path: Path) -> tuple:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"LGD master JSON not found: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    state_to_districts = defaultdict(set)\n",
    "    district_to_states = defaultdict(set)\n",
    "    \n",
    "    for entry in data:\n",
    "        raw_state = entry.get(\"StateName\", \"\")\n",
    "        raw_district = entry.get(\"DistrictName(InEnglish)\", \"\")\n",
    "        if not raw_state or not raw_district: continue\n",
    "        \n",
    "        state = lgd_normalize_text(raw_state)\n",
    "        district = lgd_normalize_text(raw_district)\n",
    "        district = LGD_DISTRICT_ALIAS.get(district, district)\n",
    "        \n",
    "        if state and district:\n",
    "            state_to_districts[state].add(district)\n",
    "            district_to_states[district].add(state)\n",
    "            \n",
    "    return dict(state_to_districts), dict(district_to_states)\n",
    "\n",
    "def lgd_validate_row(raw_state: str, raw_district: str, state_to_districts: dict, district_to_states: dict) -> tuple:\n",
    "    norm_state = lgd_normalize_text(raw_state)\n",
    "    norm_district = lgd_normalize_text(raw_district)\n",
    "\n",
    "    if not norm_state or not norm_district:\n",
    "        return False, norm_state, norm_district, \"invalid_after_normalization\"\n",
    "\n",
    "    canonical_state = LGD_STATE_ALIAS.get(norm_state, norm_state)\n",
    "    if canonical_state not in LGD_CANONICAL_STATES:\n",
    "        return False, canonical_state, norm_district, \"invalid_after_normalization\"\n",
    "    norm_state = canonical_state\n",
    "\n",
    "    norm_district = LGD_DISTRICT_ALIAS.get(norm_district, norm_district)\n",
    "    norm_state = LGD_FORCE_STATE_BY_DISTRICT.get(norm_district, norm_state)\n",
    "\n",
    "    valid_states = district_to_states.get(norm_district)\n",
    "    if not valid_states:\n",
    "        return False, norm_state, norm_district, \"district_not_in_lgd\"\n",
    "\n",
    "    if len(valid_states) == 1 and norm_state not in valid_states:\n",
    "        (norm_state,) = tuple(valid_states)\n",
    "\n",
    "    if norm_state not in valid_states:\n",
    "        reason = \"unresolved_ambiguity\" if len(valid_states) > 1 else \"district_belongs_to_other_state\"\n",
    "        return False, norm_state, norm_district, reason\n",
    "\n",
    "    if any(kw.lower() in norm_district.lower() for kw in DROP_KEYWORDS):\n",
    "        return False, norm_state, norm_district, \"invalid_after_normalization\"\n",
    "\n",
    "    return True, norm_state, norm_district, None\n",
    "\n",
    "# --- FILE PROCESSING FUNCTION ---\n",
    "\n",
    "def apply_lgd_normalization(input_root: Path, output_root: Path, logs_root: Path, chunksize: int = 100_000):\n",
    "    print(\"\\\\n--- Step 3: Applying LGD-Based Normalization and Validation ---\")\n",
    "    \n",
    "    state_to_districts, district_to_states = load_lgd_master(LGD_JSON_PATH)\n",
    "    print(f\"LGD Master Loaded: {len(district_to_states)} unique districts.\")\n",
    "\n",
    "    csv_files = sorted(list(input_root.rglob(\"*.csv\")))\n",
    "    if not csv_files:\n",
    "        print(f\"[WARN] No CSV files found in {input_root}\")\n",
    "        return\n",
    "\n",
    "    # Dictionary to hold log writers\n",
    "    log_writers = {}\n",
    "    log_files = {}\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        rel_path = csv_file.relative_to(input_root)\n",
    "        output_path = output_root / rel_path\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get the category (biometric, demographic, etc.) for segregated logging\n",
    "        category = rel_path.parts[0]\n",
    "        if category not in log_writers:\n",
    "            log_path = logs_root / f\"{category}_unresolved.csv\"\n",
    "            log_files[category] = log_path.open(\"w\", newline=\"\", encoding=\"utf-8\")\n",
    "            log_writers[category] = csv.writer(log_files[category])\n",
    "            log_writers[category].writerow([\"source_file\", \"raw_state\", \"raw_district\", \"normalized_state\", \"normalized_district\", \"drop_reason\"])\n",
    "\n",
    "        print(f\"Processing: {csv_file.name} (logging to {category}_unresolved.csv)\")\n",
    "        \n",
    "        reader = pd.read_csv(csv_file, dtype=str, chunksize=chunksize, on_bad_lines='skip', engine='python')\n",
    "        \n",
    "        is_first_chunk = True\n",
    "        for chunk in reader:\n",
    "            state_col, dist_col = _state_district_columns(chunk.columns)\n",
    "            if not state_col or not dist_col:\n",
    "                chunk.to_csv(output_path, mode='a', index=False, header=is_first_chunk)\n",
    "                is_first_chunk = False\n",
    "                continue\n",
    "\n",
    "            keep_mask = []\n",
    "            for idx, row in chunk.iterrows():\n",
    "                raw_state = row[state_col]\n",
    "                raw_dist = row[dist_col]\n",
    "                keep, norm_state, norm_dist, reason = lgd_validate_row(raw_state, raw_dist, state_to_districts, district_to_states)\n",
    "                \n",
    "                if keep:\n",
    "                    chunk.at[idx, state_col] = norm_state\n",
    "                    chunk.at[idx, dist_col] = norm_dist\n",
    "                else:\n",
    "                    log_writers[category].writerow([csv_file.name, raw_state, raw_dist, norm_state, norm_dist, reason])\n",
    "                \n",
    "                keep_mask.append(keep)\n",
    "\n",
    "            cleaned_chunk = chunk[keep_mask]\n",
    "            if not cleaned_chunk.empty:\n",
    "                cleaned_chunk.to_csv(output_path, mode='a', index=False, header=is_first_chunk)\n",
    "                is_first_chunk = False\n",
    "\n",
    "    # Close all log files\n",
    "    for f in log_files.values():\n",
    "        f.close()\n",
    "        \n",
    "    print(\"\\\\n--- Step 3 Complete ---\")\n",
    "\n",
    "# Execute the function\n",
    "apply_lgd_normalization(CLEANED_MAJOR_RULES_DIR, FINAL_OUTPUT_DIR, LOGS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18efa2f",
   "metadata": {},
   "source": [
    "## 5. Step 4: Final Cleanup\n",
    "\n",
    "This final step removes the intermediate directories (`cleaned-100k` and `cleaned_major_rules`) that were created during the pipeline's execution. This keeps the project workspace clean, leaving only the raw `data` and the `final_pipeline_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f598c42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Step 4: Cleaning up intermediate directories ---\n",
      "\\n--- Pipeline Finished ---\n",
      "Final cleaned data is located in: C:\\Users\\INDIAN\\OneDrive\\Documents\\PRO__\\uidai-datathon-2026-participation\\final_pipeline_output\n",
      "Segregated logs for dropped rows are in: C:\\Users\\INDIAN\\OneDrive\\Documents\\PRO__\\uidai-datathon-2026-participation\\final_pipeline_output\\logs\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\n--- Step 4: Cleaning up intermediate directories ---\")\n",
    "\n",
    "try:\n",
    "    if CLEANED_100K_DIR.exists():\n",
    "        shutil.rmtree(CLEANED_100K_DIR)\n",
    "        print(f\"Successfully removed: {CLEANED_100K_DIR}\")\n",
    "        \n",
    "    if CLEANED_MAJOR_RULES_DIR.exists():\n",
    "        shutil.rmtree(CLEANED_MAJOR_RULES_DIR)\n",
    "        print(f\"Successfully removed: {CLEANED_MAJOR_RULES_DIR}\")\n",
    "        \n",
    "    print(\"\\\\n--- Pipeline Finished ---\")\n",
    "    print(f\"Final cleaned data is located in: {FINAL_OUTPUT_DIR}\")\n",
    "    print(f\"Segregated logs for dropped rows are in: {LOGS_DIR}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Could not perform cleanup: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716fac00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
